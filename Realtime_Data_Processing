Project Overview

This project implements an event-driven data processing pipeline on Amazon Web Services (AWS) using Lambda, EC2, S3, DynamoDB, Auto Scaling, and CloudWatch for real-time data handling. The pipeline is triggered by a web page where users can upload files to an S3 bucket. Once uploaded, the raw data triggers a Lambda function that processes it, stores the processed data back in S3, and logs metadata in DynamoDB. IAM policies are created for Lambda and EC2 instances to manage permissions securely.

Technologies Used:

AWS Lambda: Event-driven compute service that processes raw data upon upload to S3.

AWS EC2: Virtual servers used for compute-heavy processing tasks.

Amazon S3: Object storage service for storing raw and processed data.

Amazon DynamoDB: NoSQL database for storing metadata about processed files.

AWS Auto Scaling (ASG): Dynamically adjusts EC2 instance count to handle varying loads.

AWS CloudWatch: Monitors the performance of the pipeline and triggers alerts or scaling actions.

Python Boto3: Automates AWS resource management and integration within the pipeline.

IAM Policies: Securely manage access for Lambda, EC2 instances, and DynamoDB tables.

Features:

Web Page for Upload: A user-friendly web page where users can upload files to S3, triggering the data processing pipeline.

Event-Driven Processing: Upon upload, an event triggers a Lambda function that processes the raw data and stores it as processed data in S3.

DynamoDB Metadata Storage: Stores metadata such as file ID, timestamps, and processing status in a DynamoDB table for tracking and querying.

Scalability: Auto Scaling ensures that the system can handle increasing loads by adjusting the number of EC2 instances.

IAM Policies: Secure access control for Lambda, EC2 instances, and DynamoDB using IAM policies.

Real-Time Monitoring: CloudWatch monitors key metrics like Lambda execution time, EC2 CPU utilization, and DynamoDB query performance.

Setup Instructions:

Set Up the S3 Bucket:

Create an S3 bucket for storing both raw and processed data.

Create Lambda Function:

Develop and deploy a Lambda function that processes the raw data uploaded to S3.

Include the Lambda function code to process and store the data.

Integrate Boto3 to interact with DynamoDB for metadata storage.

DynamoDB Table Setup:

Create a DynamoDB table with file_id as the partition key and upload_timestamp as the sort key.

Store metadata such as file name, processing status, and timestamps.

IAM Policies:

Create IAM policies to grant necessary permissions for Lambda, EC2, and DynamoDB interactions securely.

Web Page:

Implement the web page that allows users to upload files to S3, triggering the data processing pipeline.

Configure EC2 Instances:

Set up EC2 instances for processing compute-intensive tasks if needed.

Auto Scaling:

Configure Auto Scaling for EC2 instances to scale resources based on load.

Configure CloudWatch:

Set up CloudWatch alarms to monitor pipeline performance and trigger scaling actions.

Python Boto3:

Use Python Boto3 to automate interactions with AWS services such as Lambda, EC2, S3, and DynamoDB.

DynamoDB Implementation Details:

Table Name: processed-data-table

Primary Key: file_id (Partition Key)

Sort Key: upload_timestamp

Attributes Stored:

file_id: Unique identifier for each processed file.

upload_timestamp: Unix timestamp of when the file was uploaded.

processed_file_name: Name of the processed file stored in S3.

bucket_name: Name of the source S3 bucket.

processed_bucket_name: Name of the destination S3 bucket.

status: Processing status (e.g., "processed").

Querying:

Retrieve processed file details using file_id.

Fetch latest processed files using upload_timestamp.

Usage:

Users upload files to the S3 bucket via the web page. This triggers a Lambda function to process the raw data.

The Lambda function processes the data and stores it back in S3 as processed data.

The metadata, including the file name, timestamps, and processing status, is stored in DynamoDB.

CloudWatch monitors the system, providing performance insights and triggering scaling actions as needed.

